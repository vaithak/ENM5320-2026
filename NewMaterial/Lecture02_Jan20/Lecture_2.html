<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Universal Approximation: A Finite-Dimensional Approach - ENM5320</title>
    
    <!-- KaTeX CSS and JS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>
    
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 900px;
            margin: 0 auto;
            padding: 20px;
            background: #fff;
        }

        h1 {
            color: #011F5B;
            border-bottom: 3px solid #990000;
            padding-bottom: 10px;
            margin-top: 30px;
        }

        h2 {
            color: #011F5B;
            margin-top: 30px;
            border-bottom: 2px solid #e0e0e0;
            padding-bottom: 5px;
        }

        h3 {
            color: #011F5B;
            margin-top: 20px;
        }

        h4 {
            color: #011F5B;
            margin-top: 15px;
        }

        p {
            text-align: justify;
            margin-bottom: 15px;
        }

        ul, ol {
            margin-bottom: 15px;
        }

        li {
            margin-bottom: 8px;
        }

        .katex-display {
            margin: 1.5em 0;
            overflow-x: auto;
            overflow-y: hidden;
        }

        .definition {
            background: #f8f9fa;
            border-left: 4px solid #82AFD3;
            padding: 15px;
            margin: 20px 0;
            border-radius: 4px;
        }

        .summary-box {
            background: #fff3cd;
            border: 1px solid #F2C100;
            padding: 15px;
            margin: 20px 0;
            border-radius: 4px;
        }

        .example-box {
            background: #e8f4f8;
            border-left: 4px solid #82AFD3;
            padding: 15px;
            margin: 20px 0;
            border-radius: 4px;
        }

        .theorem-box {
            background: #f0f0f0;
            border: 2px solid #011F5B;
            padding: 15px;
            margin: 20px 0;
            border-radius: 4px;
        }

        strong {
            color: #011F5B;
        }

        .back-link {
            display: inline-block;
            margin-bottom: 20px;
            color: #82AFD3;
            text-decoration: none;
        }

        .back-link:hover {
            text-decoration: underline;
        }

        code {
            background: #f5f5f5;
            padding: 2px 6px;
            border-radius: 3px;
            font-family: 'Courier New', monospace;
            font-size: 0.9em;
        }

        pre {
            background: #f5f5f5;
            padding: 15px;
            border-radius: 4px;
            overflow-x: auto;
        }

        table {
            border-collapse: collapse;
            width: 100%;
            margin: 20px 0;
        }

        th, td {
            border: 1px solid #e0e0e0;
            padding: 10px;
            text-align: left;
        }

        th {
            background: #f8f9fa;
            color: #011F5B;
            font-weight: bold;
        }

        hr {
            border: none;
            border-top: 2px solid #e0e0e0;
            margin: 30px 0;
        }
    </style>
</head>
<body>
    <a href="../../index.html" class="back-link">← Back to Course Schedule</a>
    
    <h1>Universal Approximation: A Finite-Dimensional Approach</h1>

    <h2>0. Introduction</h2>

    <p>This note presents <strong>constructive approximation theorems</strong> for both polynomials and neural networks using primarily finite-dimensional linear algebra. We rely on the tools from our analysis review (vector norms, inner products, Cauchy-Schwarz, matrix norms) plus standard calculus results (Taylor's theorem, continuous function properties).</p>

    <div class="summary-box">
        <p><strong>Main Result (Universal Approximation):</strong> Let \(f: [a,b] \to \mathbb{R}\) be any continuous function, and let \(\epsilon > 0\) be an error tolerance. We will prove that:</p>
        <ol>
            <li><strong>Polynomials:</strong> There exists a polynomial \(p(x)\) such that 
                $$\max_{x \in [a,b]} |f(x) - p(x)| < \epsilon$$
            </li>
            <li><strong>Neural Networks:</strong> There exists a one-hidden-layer ReLU network \(f_{NN}(x)\) such that
                $$\max_{x \in [a,b]} |f(x) - f_{NN}(x)| < \epsilon$$
            </li>
        </ol>
        <p>Both approximators use \(O(\epsilon^{-c})\) parameters for some constant \(c\). The proofs are <strong>constructive</strong> - we explicitly build the approximators and bound their errors.</p>
    </div>

    <h2>1. Function Approximation on Finite Grids</h2>

    <h3>1.1 Setup: Discretizing the Problem</h3>

    <div class="definition">
        <strong>Definition (Discrete Function Space):</strong> Let \([a, b] \subset \mathbb{R}\) be a compact interval. Define a uniform grid with \(N+1\) points:
        $$x_i = a + i \cdot h, \quad i = 0, 1, \ldots, N, \quad h = \frac{b-a}{N}$$
        A <strong>discrete function</strong> is a vector \(\mathbf{f} = (f_0, f_1, \ldots, f_N)^T \in \mathbb{R}^{N+1}\) representing function values at grid points.
    </div>

    <div class="definition">
        <strong>Definition (Discrete Norms):</strong> For \(\mathbf{f} \in \mathbb{R}^{N+1}\):
        <ol>
            <li><strong>Discrete \(\ell^2\) norm:</strong>
                $$\|\mathbf{f}\|_2 = \sqrt{\sum_{i=0}^N f_i^2} $$
            </li>
            <li><strong>Discrete \(\ell^\infty\) norm:</strong>
                $$\|\mathbf{f}\|_\infty = \max_{0 \leq i \leq N} |f_i|$$
            </li>
        </ol>
    </div>

    <div class="theorem-box">
        <strong>Proposition (Norm Equivalence):</strong> For any \(\mathbf{f} \in \mathbb{R}^{N+1}\):
        $$\|\mathbf{f}\|_\infty \leq \|\mathbf{f}\|_2 \leq \sqrt{N+1} \|\mathbf{f}\|_\infty$$
        
        <p><em>Proof:</em> The first inequality follows from the definition. For the second:
        $$\|\mathbf{f}\|_2^2 = \sum_{i=0}^N f_i^2 \leq \sum_{i=0}^N \|\mathbf{f}\|_\infty^2 = (N+1) \|\mathbf{f}\|_\infty^2$$
        Taking square roots gives the result. □</p>
    </div>

    <h3>1.2 Error Measurement</h3>

    <div class="definition">
        <strong>Definition (Approximation Error):</strong> Given a target function \(\mathbf{f}\) and an approximator \(\mathbf{g}\), the approximation error is:
        $$E(\mathbf{g}, \mathbf{f}) = \|\mathbf{g} - \mathbf{f}\|$$
        where the norm can be \(\|\cdot\|_2\) or \(\|\cdot\|_\infty\) depending on the application.
    </div>

    <h2>2. Polynomial Approximation</h2>

    <h3>2.1 Lagrange Interpolation</h3>

    <div class="theorem-box">
        <strong>Theorem (Existence of Interpolating Polynomial):</strong> Given \(n\) distinct points \((x_0, y_0), \ldots, (x_{n-1}, y_{n-1})\) with \(x_i \in [a,b]\), there exists a <strong>unique</strong> polynomial \(p(x)\) of degree at most \(n-1\) such that:
        $$p(x_i) = y_i, \quad i = 0, 1, \ldots, n-1$$

        <p><em>Proof:</em></p>

        <p><strong>Step 1 (Linear System):</strong> Write \(p(x) = c_0 + c_1 x + \cdots + c_{n-1} x^{n-1}\). The interpolation conditions give:
        $$\begin{bmatrix}
        1 & x_0 & x_0^2 & \cdots & x_0^{n-1} \\
        1 & x_1 & x_1^2 & \cdots & x_1^{n-1} \\
        \vdots & \vdots & \vdots & \ddots & \vdots \\
        1 & x_{n-1} & x_{n-1}^2 & \cdots & x_{n-1}^{n-1}
        \end{bmatrix}
        \begin{bmatrix}
        c_0 \\ c_1 \\ \vdots \\ c_{n-1}
        \end{bmatrix}
        =
        \begin{bmatrix}
        y_0 \\ y_1 \\ \vdots \\ y_{n-1}
        \end{bmatrix}$$</p>

        <p><strong>Step 2 (Vandermonde Determinant):</strong> The matrix \(V\) is the <strong>Vandermonde matrix</strong>. We prove its determinant is:
        $$\det(V) = \prod_{0 \leq i < j \leq n-1} (x_j - x_i)$$
        
        <em>Proof by induction:</em>
        <ul>
            <li><strong>Base case (\(n=1\)):</strong> \(V = [1]\) has \(\det(V) = 1\), and the product over empty set is 1. ✓</li>
            <li><strong>Base case (\(n=2\)):</strong> \(V = \begin{bmatrix} 1 & x_0 \\ 1 & x_1 \end{bmatrix}\) has \(\det(V) = x_1 - x_0\). ✓</li>
            <li><strong>Inductive step:</strong> Assume the formula holds for \((n-1) \times (n-1)\) Vandermonde matrices. For the \(n \times n\) case:
                <ol>
                    <li>View \(\det(V)\) as a polynomial in \(x_{n-1}\) (the last variable). It has degree at most \(n-1\).</li>
                    <li>If \(x_{n-1} = x_i\) for any \(i < n-1\), then two rows of \(V\) are identical, so \(\det(V) = 0\). Thus \((x_{n-1} - x_i)\) is a factor for each \(i = 0, \ldots, n-2\).</li>
                    <li>Since \(\det(V)\) has degree \(n-1\) in \(x_{n-1}\) and has \(n-1\) roots, we can write:
                        $$\det(V) = C \prod_{i=0}^{n-2} (x_{n-1} - x_i)$$
                        for some constant \(C\) (which may depend on \(x_0, \ldots, x_{n-2}\)).</li>
                    <li>To find \(C\), look at the coefficient of \(x_{n-1}^{n-1}\) in \(\det(V)\). By cofactor expansion along the last row, this coefficient comes from the \((n-1) \times (n-1)\) Vandermonde minor obtained by deleting the last row and column, which by induction has determinant:
                        $$C = \prod_{0 \leq i < j \leq n-2} (x_j - x_i)$$</li>
                    <li>Combining gives the full formula. □</li>
                </ol>
            </li>
        </ul>
        
        Since all \(x_i\) are distinct, \(\det(V) \neq 0\), so \(V\) is invertible.</p>

        <p><strong>Step 3 (Unique Solution):</strong> The coefficient vector is \(\mathbf{c} = V^{-1} \mathbf{y}\), which is unique. □</p>
    </div>

    <p><strong>Corollary (Zero Error on Grid):</strong> The interpolating polynomial achieves:
    $$\|\mathbf{p} - \mathbf{f}\|_2 = \|\mathbf{p} - \mathbf{f}\|_\infty = 0$$
    on the discrete grid points.</p>

    <h3>2.2 Explicit Construction: Lagrange Basis</h3>

    <div class="definition">
        <strong>Definition (Lagrange Basis Polynomials):</strong> For each \(j = 0, \ldots, n-1\), define:
        $$L_j(x) = \prod_{k=0, k \neq j}^{n-1} \frac{x - x_k}{x_j - x_k}$$

        <p><strong>Properties:</strong></p>
        <ol>
            <li>\(L_j(x_i) = \delta_{ij}\) (Kronecker delta)</li>
            <li>\(\deg(L_j) = n-1\)</li>
            <li>\(\sum_{j=0}^{n-1} L_j(x) = 1\) for all \(x\)</li>
        </ol>
    </div>

    <div class="theorem-box">
        <strong>Theorem (Lagrange Interpolation Formula):</strong>
        $$p(x) = \sum_{j=0}^{n-1} y_j L_j(x)$$

        <p><em>Proof:</em> Direct verification:
        $$p(x_i) = \sum_{j=0}^{n-1} y_j L_j(x_i) = \sum_{j=0}^{n-1} y_j \delta_{ij} = y_i$$
        The polynomial has degree at most \(n-1\) and satisfies all interpolation conditions, so by uniqueness, this is the interpolating polynomial. □</p>
    </div>

    <h3>2.3 Polynomial Approximation (Not Interpolation)</h3>

    <p>The Lagrange interpolation approach forces exact agreement at grid points, which can lead to oscillations between points (the <strong>Runge phenomenon</strong>). However, if we allow small errors at grid points, polynomials can achieve excellent <strong>uniform</strong> approximation.</p>

    <div style="text-align: center; margin: 30px 0;">
        <img src="runge_phenomenon.png" alt="Runge Phenomenon Comparison" style="max-width: 100%; height: auto; border: 1px solid #e0e0e0; border-radius: 4px;">
        <p style="margin-top: 10px; font-style: italic; color: #666; text-align: justify;"><strong>Figure 1:</strong> Comparison of approximation methods for the Runge function \(f(x) = \frac{1}{1 + 25x^2}\) with \(n=11\) nodes. <strong>(a)</strong> Lagrange interpolation on equally-spaced nodes exhibits large oscillations near the boundaries (Runge phenomenon). <strong>(b)</strong> Lagrange interpolation on Chebyshev nodes converges stably without oscillations. <strong>(c)</strong> Bernstein polynomials (degree 33) provide uniform convergence. <strong>(d)</strong> Error comparison on log scale demonstrates that uniform interpolation diverges while Chebyshev and Bernstein methods converge.</p>
    </div>

    <h3>2.4 Uniform Polynomial Approximation with Error Control</h3>

    <p>Instead of requiring \(p(x_i) = f_i\) exactly, we can construct polynomials that minimize the <strong>worst-case error</strong> over the entire interval.</p>

    <div class="theorem-box">
        <strong>Theorem (Constructive Polynomial Approximation):</strong> Let \(f: [a,b] \to \mathbb{R}\) be continuous. For any \(\epsilon > 0\), there exists a Bernstein polynomial \(B_n(f)\) of degree \(n = O(\epsilon^{-2})\) such that:
        $$\max_{x \in [a,b]} |f(x) - B_n(f; x)| < \epsilon$$
        
        For smooth functions \(f \in C^2[a,b]\) with \(|f''(x)| \leq M\), the convergence rate is \(O(n^{-1})\).

        <p><em>Proof (Constructive via Bernstein Polynomials):</em></p>

        <p><strong>Step 1 (Bernstein Basis):</strong> For \(f\) on \([0,1]\), define the \(n\)-th Bernstein polynomial:
        $$B_n(f; x) = \sum_{k=0}^n f\left(\frac{k}{n}\right) \binom{n}{k} x^k (1-x)^{n-k}$$
        This is a polynomial of degree \(n\). The basis functions \(b_{n,k}(x) = \binom{n}{k} x^k (1-x)^{n-k}\) are non-negative and satisfy \(\sum_{k=0}^n b_{n,k}(x) = 1\) (partition of unity from the binomial theorem).</p>

        <p><strong>Step 2 (Key Properties):</strong> The Bernstein basis satisfies:
        <ul>
            <li>\(\sum_{k=0}^n b_{n,k}(x) = (x + (1-x))^n = 1\) (partition of unity)</li>
            <li>\(\sum_{k=0}^n \frac{k}{n} b_{n,k}(x) = x\) (can be verified by differentiation)</li>
            <li>\(\sum_{k=0}^n \left(\frac{k}{n} - x\right)^2 b_{n,k}(x) \leq \frac{1}{4n}\) (variance bound, proved using the above identities)</li>
        </ul></p>

        <p><strong>Step 3 (Error Decomposition):</strong> For any \(x \in [0,1]\):
        $$|f(x) - B_n(f; x)| = \left|\sum_{k=0}^n \left[f(x) - f\left(\frac{k}{n}\right)\right] b_{n,k}(x)\right|$$</p>

        <p><strong>Step 4 (Smoothness Bound):</strong> By Taylor expansion, for \(|t - x| \leq \delta\):
        $$\left|f(t) - f(x)\right| \leq |f'(x)||t-x| + \frac{M}{2}|t-x|^2 \leq \|f'\|_\infty \delta + \frac{M}{2}\delta^2$$</p>

        <p><strong>Step 5 (Uniform Continuity):</strong> For continuous \(f\) on \([0,1]\), given \(\epsilon > 0\), there exists \(\delta > 0\) such that:
        $$|f(x) - f(y)| < \frac{\epsilon}{2} \quad \text{whenever } |x - y| < \delta$$</p>

        <p><strong>Step 6 (Split Sum):</strong> Split the sum into "close" and "far" terms:
        $$|f(x) - B_n(f; x)| \leq \sum_{|k/n - x| < \delta} + \sum_{|k/n - x| \geq \delta}$$</p>

        <p><strong>Step 7 (Close Terms):</strong> When \(|k/n - x| < \delta\), by uniform continuity:
        $$\sum_{|k/n - x| < \delta} \left|f(x) - f\left(\frac{k}{n}\right)\right| b_{n,k}(x) < \frac{\epsilon}{2} \sum_k b_{n,k}(x) = \frac{\epsilon}{2}$$</p>

        <p><strong>Step 8 (Far Terms):</strong> For \(|k/n - x| \geq \delta\):
        $$\sum_{|k/n - x| \geq \delta} \left|f(x) - f\left(\frac{k}{n}\right)\right| b_{n,k}(x) \leq 2\|f\|_\infty \sum_{|k/n - x| \geq \delta} b_{n,k}(x)$$
        
        To bound the sum, note that for \(|k/n - x| \geq \delta\), we have \((k/n - x)^2 \geq \delta^2\). Therefore:
        $$\delta^2 \sum_{|k/n - x| \geq \delta} b_{n,k}(x) \leq \sum_{|k/n - x| \geq \delta} (k/n - x)^2 b_{n,k}(x) \leq \sum_{k=0}^n (k/n - x)^2 b_{n,k}(x) \leq \frac{1}{4n}$$
        
        Thus:
        $$\sum_{|k/n - x| \geq \delta} b_{n,k}(x) \leq \frac{1}{4n\delta^2}$$
        
        So the far terms contribute:
        $$2\|f\|_\infty \cdot \frac{1}{4n\delta^2} = \frac{\|f\|_\infty}{2n\delta^2}$$</p>

        <p><strong>Step 9 (Combine):</strong> Therefore:
        $$|f(x) - B_n(f; x)| < \frac{\epsilon}{2} + \frac{\|f\|_\infty}{2n\delta^2}$$
        Choose \(n \geq \frac{\|f\|_\infty}{\delta^2 \epsilon}\) to make the second term less than \(\epsilon/2\), giving total error \(< \epsilon\).</p>
        
        <p><strong>For smooth functions:</strong> If \(f \in C^2\) with \(|f''| \leq M\), we can take \(\delta = \sqrt{\epsilon/(2M)}\) from Taylor expansion, giving \(n = O(\epsilon^{-2})\). □</p>
    </div>

    <p><strong>Corollary:</strong> For any continuous \(f\) on \([a,b]\) and any \(\epsilon > 0\), there exists a polynomial with degree \(n = O(\epsilon^{-2})\) achieving \(\|f - p\|_\infty < \epsilon\).</p>

    <h3>2.5 Alternative: Chebyshev Nodes for Interpolation</h3>

    <p>Another approach is to use <strong>non-uniform</strong> grid points that avoid Runge phenomenon.</p>

    <div class="definition">
        <strong>Definition (Chebyshev Nodes):</strong> On \([-1, 1]\), the \(n\)-th Chebyshev nodes are:
        $$x_k = \cos\left(\frac{(2k+1)\pi}{2n}\right), \quad k = 0, 1, \ldots, n-1$$
        These points cluster near the boundaries.
    </div>

    <div class="theorem-box">
        <strong>Theorem (Chebyshev Interpolation Error):</strong> For \(f \in C^k[-1,1]\), the Lagrange interpolation polynomial at Chebyshev nodes satisfies:
        $$\max_{x \in [-1,1]} |f(x) - p(x)| \leq \frac{2}{k!} \left(\frac{1}{n}\right)^k \|f^{(k)}\|_\infty$$

        <p><em>Key Insight:</em> Unlike equally-spaced points, Chebyshev interpolation <strong>converges</strong> for smooth functions, with error \(O(n^{-k})\) for \(f \in C^k\).</p>
    </div>

    <h3>2.6 Remark on the Runge Phenomenon</h3>

    <p><strong>Important Clarification:</strong> The Runge phenomenon occurs for polynomial <strong>interpolation on equally-spaced points</strong>. It does NOT mean polynomials are inferior approximators! With proper choices:</p>
    <ul>
        <li><strong>Bernstein polynomials:</strong> Uniform convergence for any continuous function</li>
        <li><strong>Chebyshev interpolation:</strong> Exponential convergence for smooth functions</li>
        <li><strong>Least-squares approximation:</strong> Optimal in \(L^2\) norm</li>
    </ul>

    <p><strong>Moral:</strong> Both polynomials and neural networks require careful construction. The choice of grid/nodes matters as much as the choice of basis functions!</p>

    <h2>3. Neural Network Approximation</h2>

    <h3>3.1 One-Hidden-Layer Networks</h3>

    <div class="definition">
        <strong>Definition (Shallow Neural Network):</strong> A one-hidden-layer network with \(m\) neurons, ReLU activation, is a function:
        $$f_{NN}(x) = \sum_{j=1}^m w_j \sigma(a_j x + b_j) + c$$
        where:
        <ul>
            <li>\(\sigma(z) = \max(0, z)\) is the ReLU activation</li>
            <li>\(a_j, b_j, w_j, c \in \mathbb{R}\) are learnable parameters</li>
        </ul>

        <p><strong>Total Parameters:</strong> \(3m + 1\)</p>
    </div>

    <h3>3.2 Building Blocks: ReLU Functions</h3>

    <p><strong>Lemma (ReLU Combinations):</strong></p>

    <ol>
        <li><strong>Ramp function:</strong> \(\text{ramp}(x; x_0, x_1) = \sigma(x - x_0) - \sigma(x - x_1)\)
            <ul>
                <li>Zero outside \([x_0, x_1]\)</li>
                <li>Linear inside \([x_0, x_1]\)</li>
            </ul>
        </li>
        <li><strong>Hat function:</strong>
            $$\phi_i(x) = \begin{cases}
            1 - \frac{|x - x_i|}{h} & \text{if } |x - x_i| \leq h \\
            0 & \text{otherwise}
            \end{cases}$$
            This can be written as:
            $$\phi_i(x) = \frac{1}{h}[\sigma(h(x - x_{i-1})) - 2\sigma(h(x - x_i)) + \sigma(h(x - x_{i+1}))]$$
        </li>
    </ol>

    <div class="theorem-box">
        <strong>Proof of Hat Function Construction:</strong>

        <p><strong>Step 1:</strong> Define the building blocks:
        <ul>
            <li>\(\sigma(h(x - x_{i-1}))\) creates a ramp starting at \(x_{i-1}\)</li>
            <li>\(-2\sigma(h(x - x_i))\) creates a downward ramp starting at \(x_i\)</li>
            <li>\(\sigma(h(x - x_{i+1}))\) creates an upward ramp starting at \(x_{i+1}\)</li>
        </ul></p>

        <p><strong>Step 2:</strong> Verify at key points:
        <ul>
            <li>At \(x = x_{i-1}\): \(\phi_i = \frac{1}{h}[0 - 0 + 0] = 0\)</li>
            <li>At \(x = x_i\): \(\phi_i = \frac{1}{h}[h - 0 + 0] = 1\)</li>
            <li>At \(x = x_{i+1}\): \(\phi_i = \frac{1}{h}[2h - 2h + 0] = 0\)</li>
        </ul></p>

        <p><strong>Step 3:</strong> Check linearity in each interval by differentiation:
        $$\phi_i'(x) = \begin{cases}
        1/h & x \in (x_{i-1}, x_i) \\
        -1/h & x \in (x_i, x_{i+1}) \\
        0 & \text{otherwise}
        \end{cases}$$
        This gives the piecewise linear hat shape. □</p>
    </div>

    <h3>3.3 Piecewise Linear Interpolation with Neural Networks</h3>

    <div class="theorem-box">
        <strong>Theorem (Neural Network Interpolation):</strong> Given grid points \((x_0, f_0), \ldots, (x_N, f_N)\) on \([a,b]\), there exists a one-hidden-layer ReLU network with \(O(N)\) neurons such that:
        $$f_{NN}(x_i) = f_i, \quad i = 0, 1, \ldots, N$$
        and \(f_{NN}\) is piecewise linear between grid points.

        <p><em>Proof (Construction):</em></p>

        <p><strong>Step 1 (Basis Expansion):</strong> Define:
        $$f_{NN}(x) = \sum_{i=0}^N f_i \phi_i(x)$$
        where \(\phi_i\) are the hat functions from the previous lemma.</p>

        <p><strong>Step 2 (Verification at Grid Points):</strong> By the Kronecker property \(\phi_i(x_j) = \delta_{ij}\):
        $$f_{NN}(x_j) = \sum_{i=0}^N f_i \delta_{ij} = f_j$$</p>

        <p><strong>Step 3 (Neuron Count):</strong> Each interior hat function \(\phi_i\) (\(i = 1, \ldots, N-1\)) requires 3 ReLU neurons. The boundary hat functions \(\phi_0\) and \(\phi_N\) each require 2 neurons. Total:
        $$3(N-1) + 2 \cdot 2 = 3N - 3 + 4 = 3N + 1 \text{ ReLU activations}$$
        (Plus bias adjustments, giving approximately \(3N\) neurons). □</p>
    </div>

    <p><strong>Corollary (Zero Error on Grid):</strong> Like polynomial interpolation:
    $$\|\mathbf{f}_{NN} - \mathbf{f}\|_2 = \|\mathbf{f}_{NN} - \mathbf{f}\|_\infty = 0$$
    on the discrete grid.</p>

    <h3>3.4 Between-Grid Behavior</h3>

    <p><strong>Key Difference from Polynomials:</strong> The neural network approximation \(f_{NN}\) is:</p>
    <ol>
        <li><strong>Continuous</strong> (even though ReLU is not differentiable at kinks)</li>
        <li><strong>Piecewise linear</strong> between grid points</li>
        <li><strong>Bounded</strong> by \(\|\mathbf{f}\|_\infty\) on the entire interval</li>
    </ol>

    <div class="theorem-box">
        <strong>Proposition (Boundedness):</strong> For the constructed \(f_{NN}\):
        $$\|f_{NN}\|_{C[a,b]} = \max_{x \in [a,b]} |f_{NN}(x)| \leq \|\mathbf{f}\|_\infty$$

        <p><em>Proof:</em> Since \(\sum_i \phi_i(x) = 1\) for all \(x \in [a,b]\) (partition of unity):
        $$|f_{NN}(x)| = \left|\sum_i f_i \phi_i(x)\right| \leq \sum_i |f_i| \phi_i(x) \leq \|\mathbf{f}\|_\infty \sum_i \phi_i(x) = \|\mathbf{f}\|_\infty$$
        by the triangle inequality and non-negativity of \(\phi_i\). □</p>
    </div>

    <p><strong>Remark:</strong> This boundedness property prevents Runge-like oscillations. The neural network cannot "explode" between grid points.</p>

    <h2>4. Comparison and Error Analysis</h2>

    <h3>4.1 Approximation of Smooth Functions</h3>

    <p><strong>Assumption:</strong> Suppose \(f \in C^2[a,b]\) with \(|f''(x)| \leq M\) for all \(x \in [a,b]\).</p>

    <div class="theorem-box">
        <strong>Theorem (Piecewise Linear Approximation Error):</strong> For the neural network \(f_{NN}\) constructed from grid values:
        $$\max_{x \in [a,b]} |f(x) - f_{NN}(x)| \leq \frac{M h^2}{8}$$
        where \(h = (b-a)/N\) is the grid spacing.

        <p><em>Proof:</em></p>

        <p><strong>Step 1 (Localization):</strong> It suffices to bound the error on each subinterval \([x_i, x_{i+1}]\).</p>

        <p><strong>Step 2 (Linear Interpolation on Subinterval):</strong> On \([x_i, x_{i+1}]\), the neural network is:
        $$f_{NN}(x) = f_i + \frac{f_{i+1} - f_i}{h}(x - x_i)$$</p>

        <p><strong>Step 3 (Standard Interpolation Error Formula):</strong> For a twice-differentiable function \(f\) on \([x_i, x_{i+1}]\), a standard result from calculus/numerical analysis states that the error in linear interpolation is given by:
        $$f(x) - f_{NN}(x) = \frac{(x - x_i)(x - x_{i+1})}{2} f''(\xi)$$
        for some \(\xi \in (x_i, x_{i+1})\). This follows from Taylor's theorem applied to both endpoints.</p>

        <p><strong>Step 4 (Bound the Product):</strong> On \([x_i, x_{i+1}]\), we have \(x - x_i \geq 0\) and \(x - x_{i+1} \leq 0\), so:
        $$(x - x_i)(x - x_{i+1}) \leq 0$$
        Taking absolute value:
        $$|(x - x_i)(x - x_{i+1})| = (x - x_i)(x_{i+1} - x) = (x - x_i)(h - (x - x_i))$$</p>

        <p><strong>Step 5 (Maximize Product):</strong> Let \(s = x - x_i \in [0, h]\). Maximize \(g(s) = s(h - s) = sh - s^2\). Taking derivative:
        $$g'(s) = h - 2s = 0 \implies s = h/2$$
        The maximum is:
        $$g(h/2) = \frac{h}{2} \cdot \frac{h}{2} = \frac{h^2}{4}$$</p>

        <p><strong>Step 6 (Apply Bound):</strong> Using \(|f''(\xi)| \leq M\):
        $$|f(x) - f_{NN}(x)| = \frac{|(x - x_i)(x - x_{i+1})|}{2} |f''(\xi)| \leq \frac{1}{2} \cdot \frac{h^2}{4} \cdot M = \frac{M h^2}{8}$$
        □</p>
    </div>

    <p><strong>Corollary (Convergence Rate):</strong> As \(N \to \infty\) (equivalently \(h \to 0\)):
    $$\max_{x \in [a,b]} |f(x) - f_{NN}(x)| = O(N^{-2})$$</p>

    <p>This is a <strong>second-order</strong> convergence rate, which is excellent for practical approximation.</p>

    <h3>4.2 Polynomial Approximation Error (Complete Picture)</h3>

    <p><strong>Interpolation vs Approximation:</strong> We must distinguish two approaches:</p>

    <div class="example-box">
        <strong>Approach 1: Polynomial Interpolation on Equally-Spaced Points</strong>
        <p>Error can be bounded using:
        $$|f(x) - p(x)| \leq \frac{|f^{(n)}(\xi)|}{n!} \prod_{i=0}^{n-1} |x - x_i|$$</p>

        <p><strong>Issue:</strong> The term \(\prod_i |x - x_i|\) can grow as \((b-a)^n / 4^n\), leading to divergence (Runge phenomenon).</p>
    </div>

    <div class="example-box">
        <strong>Approach 2: Polynomial Approximation (Bernstein/Chebyshev)</strong>
        <p>For Bernstein polynomials of degree \(n\):
        $$\max_{x \in [a,b]} |f(x) - B_n(f; x)| \leq \frac{C}{n}$$
        for \(f \in C^2\), where \(C\) depends only on \(\|f''\|_\infty\) and the interval length.</p>

        <p><strong>Result:</strong> Guaranteed convergence as \(n \to \infty\) for any continuous function!</p>
    </div>

    <p><strong>Fair Comparison:</strong></p>
    <table style="margin: 20px 0;">
        <thead>
            <tr>
                <th>Method</th>
                <th>Convergence Rate</th>
                <th>Requirements</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>Polynomial (Bernstein)</td>
                <td>\(O(n^{-1})\) for \(C^2\)</td>
                <td>Continuous \(f\)</td>
            </tr>
            <tr>
                <td>Polynomial (Chebyshev)</td>
                <td>\(O(n^{-k})\) for \(C^k\)</td>
                <td>Non-uniform nodes</td>
            </tr>
            <tr>
                <td>Neural Network (piecewise linear)</td>
                <td>\(O(n^{-2})\) for \(C^2\)</td>
                <td>Continuous \(f\)</td>
            </tr>
        </tbody>
    </table>

    <p><strong>Key Insight:</strong> Both methods achieve universal approximation! The convergence rates differ, but both can approximate any continuous function to arbitrary precision with enough parameters.</p>

    <h2>5. Universal Approximation Statements</h2>

    <h3>5.1 Discrete Universal Approximation</h3>

    <div class="theorem-box">
        <strong>Theorem (Finite-Dimensional Approximation):</strong> Let \(\mathbf{f} \in \mathbb{R}^{N+1}\) be any discrete function on a grid of \(N+1\) points. Then:

        <ol>
            <li><strong>Polynomial Version (Interpolation):</strong> There exists a polynomial \(p\) of degree at most \(N\) such that:
                $$\|\mathbf{p} - \mathbf{f}\|_2 = \|\mathbf{p} - \mathbf{f}\|_\infty = 0$$
                (exact at grid points via Lagrange interpolation)
            </li>
            <li><strong>Neural Network Version (Interpolation):</strong> There exists a one-hidden-layer ReLU network with \(O(N)\) neurons such that:
                $$\|\mathbf{f}_{NN} - \mathbf{f}\|_2 = \|\mathbf{f}_{NN} - \mathbf{f}\|_\infty = 0$$
                (exact at grid points via hat functions)
            </li>
        </ol>

        <p><em>Proof:</em> Both are established by the interpolation theorems above. □</p>
    </div>

    <p><strong>Remark:</strong> This theorem states that both polynomial and neural network function classes are <strong>equally powerful</strong> for representing discrete data. Both achieve zero error on grids with \(O(N)\) parameters.</p>

    <h3>5.2 Approximate Universal Approximation (With Error Tolerance)</h3>

    <div class="theorem-box">
        <strong>Practical Theorem (Universal Approximation):</strong> Let \(f: [a,b] \to \mathbb{R}\) be continuous. For any \(\epsilon > 0\), there exist approximators with \(O(\epsilon^{-c})\) parameters such that:

        <ol>
            <li><strong>Polynomial (Bernstein):</strong> A polynomial of degree \(n = O(\epsilon^{-2})\) satisfies:
                $$\|f - B_n(f)\|_{C[a,b]} < \epsilon$$
            </li>
            <li><strong>Polynomial (Chebyshev interpolation):</strong> For smooth \(f \in C^k\), degree \(n = O(\epsilon^{-1/k})\) suffices:
                $$\|f - p\|_{C[a,b]} < \epsilon$$
            </li>
            <li><strong>Neural Network (piecewise linear):</strong> For \(f \in C^2\), a ReLU network with \(n = O(\epsilon^{-1/2})\) neurons satisfies:
                $$\|f - f_{NN}\|_{C[a,b]} < \epsilon$$
            </li>
        </ol>

        <p><em>Proof Sketch:</em></p>
        <ul>
            <li><strong>Polynomials:</strong> Bernstein polynomial construction (Section 2.4) gives the rate. Chebyshev interpolation improves the rate for smooth functions.</li>
            <li><strong>Neural Networks:</strong> Choose grid spacing \(h = (b-a)/N\) such that \(Mh^2/8 < \epsilon\), giving \(N = O(\epsilon^{-1/2})\). The constructed network uses \(O(N)\) neurons. □</li>
        </ul>
    </div>

    <p><strong>Critical Observation:</strong> Both polynomials and neural networks achieve <strong>universal approximation</strong>:</p>
    <ul>
        <li>Any continuous function can be approximated to arbitrary precision</li>
        <li>The number of parameters scales polynomially with \(1/\epsilon\)</li>
        <li>The choice between them depends on the specific problem structure</li>
    </ul>

    <h2>6. Vectorized Framework</h2>

    <h3>6.1 Matrix Representation of Networks</h3>

    <p><strong>Network as Matrix Operation:</strong> A one-hidden-layer network can be written as:
    $$f_{NN}(x) = \mathbf{w}^T \sigma(\mathbf{A} x + \mathbf{b}) + c$$
    where:
    <ul>
        <li>\(\mathbf{A} \in \mathbb{R}^{m \times d}\) is the input weight matrix</li>
        <li>\(\mathbf{b} \in \mathbb{R}^m\) is the bias vector</li>
        <li>\(\sigma\) is applied element-wise</li>
        <li>\(\mathbf{w} \in \mathbb{R}^m\) is the output weight vector</li>
    </ul></p>

    <div class="theorem-box">
        <strong>Proposition (Network Output Bound):</strong> For input \(x \in \mathbb{R}^d\) with \(\|x\|_2 \leq R\):
        $$|f_{NN}(x)| \leq \|\mathbf{w}\|_1 \|\sigma(\mathbf{A} x + \mathbf{b})\|_\infty + |c|$$

        <p><em>Proof:</em> By Hölder's inequality (dual norms):
        $$|\mathbf{w}^T \mathbf{z}| \leq \|\mathbf{w}\|_1 \|\mathbf{z}\|_\infty$$
        where \(\mathbf{z} = \sigma(\mathbf{A} x + \mathbf{b})\). Add the bias term \(c\). □</p>
    </div>

    <h3>6.2 Polynomial as Matrix Operation</h3>

    <p><strong>Vandermonde Matrix Formulation:</strong> Evaluation of polynomial \(p(x) = \sum_{j=0}^{n-1} c_j x^j\) at grid points can be written as:
    $$\mathbf{p} = V \mathbf{c}$$
    where \(V_{ij} = x_i^j\) is the Vandermonde matrix.</p>

    <div class="theorem-box">
        <strong>Proposition (Polynomial Coefficient Bound):</strong> If \(\|\mathbf{p}\|_\infty \leq M\), then:
        $$\|\mathbf{c}\|_2 \leq \|V^{-1}\|_2 \sqrt{n} M$$

        <p><em>Proof:</em> From \(\mathbf{c} = V^{-1} \mathbf{p}\):
        $$\|\mathbf{c}\|_2 \leq \|V^{-1}\|_2 \|\mathbf{p}\|_2 \leq \|V^{-1}\|_2 \sqrt{n} \|\mathbf{p}\|_\infty$$
        using submultiplicativity and norm equivalence. □</p>
    </div>

    <p><strong>Remark:</strong> The norm \(\|V^{-1}\|_2\) can be very large for equally spaced points, explaining the numerical instability of high-degree polynomial interpolation.</p>

    <h2>7. Computational Exercises</h2>

    <div class="example-box">
        <strong>Exercise 1: Lagrange Interpolation</strong>

        <p><strong>Task:</strong> Implement Lagrange interpolation for \(f(x) = \sin(\pi x)\) on \([0,1]\) with \(n = 5, 10, 20\) equally spaced points.</p>

        <p><strong>Questions:</strong></p>
        <ol>
            <li>Compute \(\|\mathbf{p} - \mathbf{f}\|_2\) on the grid points (should be machine-precision zero)</li>
            <li>Evaluate \(p(x)\) at 1000 equally spaced points in \([0,1]\)</li>
            <li>Compute \(\max_{x \in [0,1]} |p(x) - \sin(\pi x)|\)</li>
            <li>Plot the error as a function of \(n\)</li>
        </ol>
    </div>

    <div class="example-box">
        <strong>Exercise 2: ReLU Network Construction</strong>

        <p><strong>Task:</strong> Build a piecewise linear interpolator using hat functions for \(f(x) = e^x\) on \([-1,1]\).</p>

        <p><strong>Implementation:</strong></p>
        <ol>
            <li>Generate grid with \(N = 10\) points</li>
            <li>Construct hat functions \(\phi_i(x)\) using ReLU</li>
            <li>Form \(f_{NN}(x) = \sum_i f_i \phi_i(x)\)</li>
            <li>Verify \(f_{NN}(x_i) = e^{x_i}\) for all grid points</li>
        </ol>

        <p><strong>Analysis:</strong></p>
        <ol>
            <li>Compute maximum error between grid points</li>
            <li>Compare with the theoretical bound \(Mh^2/8\)</li>
            <li>Plot both \(f\) and \(f_{NN}\)</li>
        </ol>
    </div>

    <div class="example-box">
        <strong>Exercise 3: Runge Phenomenon</strong>

        <p><strong>Task:</strong> Compare polynomial and neural network approximation of the Runge function:
        $$f(x) = \frac{1}{1 + 25x^2}, \quad x \in [-1, 1]$$</p>

        <p><strong>Procedure:</strong></p>
        <ol>
            <li>Use \(N = 10, 15, 20\) equally spaced points</li>
            <li>Compute Lagrange polynomial \(p(x)\)</li>
            <li>Compute piecewise linear \(f_{NN}(x)\)</li>
            <li>For each, evaluate at 1000 test points</li>
            <li>Plot \(\max |f - p|\) and \(\max |f - f_{NN}|\) vs. \(N\)</li>
        </ol>

        <p><strong>Expected Result:</strong> Polynomial error grows with \(N\), neural network error decreases as \(O(N^{-2})\).</p>
    </div>

    <div class="example-box">
        <strong>Exercise 4: Norm Analysis</strong>

        <p><strong>Task:</strong> For a random vector \(\mathbf{f} \in \mathbb{R}^{50}\):</p>

        <ol>
            <li>Verify \(\|\mathbf{f}\|_\infty \leq \|\mathbf{f}\|_2 \leq \sqrt{50}\|\mathbf{f}\|_\infty\)</li>
            <li>Compute Vandermonde matrix \(V\) for \(N = 50\) equally spaced points in \([0,1]\)</li>
            <li>Estimate \(\kappa(V) = \|V\|_2 \|V^{-1}\|_2\) (condition number)</li>
            <li>Observe how \(\kappa(V)\) grows with \(N\)</li>
        </ol>
    </div>

    <h2>8. Connection to Continuous Case</h2>

    <h3>8.1 Taking the Limit \(N \to \infty\)</h3>

    <p>The finite-dimensional theorems naturally extend to the infinite-dimensional setting:</p>

    <p><strong>Discrete → Continuous:</strong></p>
    <ul>
        <li>\(\mathbb{R}^{N+1} \to C[a,b]\) (continuous functions)</li>
        <li>\(\|\cdot\|_2 \to \|f\|_{L^2} = \sqrt{\int_a^b f(x)^2 \, dx}\)</li>
        <li>\(\|\cdot\|_\infty \to \|f\|_{C[a,b]} = \max_{x \in [a,b]} |f(x)|\)</li>
    </ul>

    <p><strong>Interpolation → Approximation:</strong></p>
    <ul>
        <li>Exact interpolation becomes approximate with error \(O(h^k)\)</li>
        <li>Convergence rate depends on smoothness of \(f\)</li>
        <li>For \(f \in C^k\), error is \(O(N^{-k})\) for neural networks</li>
    </ul>

    <p><strong>Classical Results:</strong></p>
    <ol>
        <li><strong>Weierstrass Approximation Theorem:</strong> Polynomials are dense in \(C[a,b]\)</li>
        <li><strong>Cybenko's Theorem (1989):</strong> One-hidden-layer networks with sigmoid activations are universal approximators</li>
        <li><strong>Modern Extensions:</strong> ReLU networks, deep networks, approximation rates</li>
    </ol>

    <p><strong>What We've Shown:</strong> The finite-dimensional versions give:</p>
    <ul>
        <li>Constructive proofs (not just existence)</li>
        <li>Explicit convergence rates</li>
        <li>Computational implementation</li>
        <li>Intuition for the continuous case</li>
    </ul>

    <hr>

    <h2>Summary</h2>

    <table>
        <thead>
            <tr>
                <th>Property</th>
                <th>Polynomials</th>
                <th>Neural Networks</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td><strong>Exact on grid</strong></td>
                <td>✓ (Lagrange)</td>
                <td>✓ (hat functions)</td>
            </tr>
            <tr>
                <td><strong>Universal approximation</strong></td>
                <td>✓ (Weierstrass/Bernstein)</td>
                <td>✓ (Cybenko)</td>
            </tr>
            <tr>
                <td><strong>Convergence rate (smooth f)</strong></td>
                <td>\(O(n^{-k})\) for \(C^k\)</td>
                <td>\(O(n^{-2})\) for \(C^2\)</td>
            </tr>
            <tr>
                <td><strong>Parameters for \(N\) points</strong></td>
                <td>\(N+1\) coefficients</td>
                <td>\(\sim 3N\) neurons</td>
            </tr>
            <tr>
                <td><strong>Construction methods</strong></td>
                <td>Lagrange, Bernstein, Chebyshev</td>
                <td>ReLU combinations, hat functions</td>
            </tr>
            <tr>
                <td><strong>Numerical stability</strong></td>
                <td>Depends on nodes (Chebyshev good)</td>
                <td>Good (local basis)</td>
            </tr>
            <tr>
                <td><strong>Smoothness</strong></td>
                <td>\(C^\infty\)</td>
                <td>\(C^0\) (piecewise linear)</td>
            </tr>
            <tr>
                <td><strong>Runge phenomenon</strong></td>
                <td>Only with equally-spaced interpolation</td>
                <td>Not applicable (piecewise)</td>
            </tr>
        </tbody>
    </table>

    <div class="summary-box">
        <p><strong>Key Takeaway:</strong> Both polynomials and neural networks are <strong>universal approximators</strong> with similar theoretical justification:</p>
        <ul>
            <li><strong>Exact representation:</strong> Both can exactly represent any finite discrete function</li>
            <li><strong>Uniform convergence:</strong> Both can approximate any continuous function to arbitrary precision</li>
            <li><strong>Parameter efficiency:</strong> Both use \(O(N)\) parameters for \(N\)-point data</li>
            <li><strong>Practical differences:</strong> Polynomials have global smoothness; neural networks have local structure</li>
        </ul>
        <p><strong>Don't automatically assume DNNs are superior!</strong> The choice depends on problem structure, smoothness requirements, and computational constraints. Classical polynomial methods remain highly effective for many applications.</p>
    </div>

    <h2>References for Further Study</h2>

    <ol>
        <li><strong>Finite-Dimensional Analysis:</strong>
            <ul>
                <li>Trefethen, <em>Approximation Theory and Approximation Practice</em> (2013)</li>
                <li>Interpolation and approximation with explicit error bounds</li>
            </ul>
        </li>
        <li><strong>Neural Network Theory:</strong>
            <ul>
                <li>Pinkus, "Approximation theory of the MLP model in neural networks" (1999)</li>
                <li>Constructive approximation with ReLU networks</li>
            </ul>
        </li>
        <li><strong>Classical Results:</strong>
            <ul>
                <li>Cybenko, "Approximation by superpositions of a sigmoidal function" (1989)</li>
                <li>Hornik, "Multilayer feedforward networks are universal approximators" (1989)</li>
            </ul>
        </li>
        <li><strong>Modern Perspectives:</strong>
            <ul>
                <li>Deep learning theory and approximation rates</li>
                <li>Connection between width, depth, and approximation power</li>
            </ul>
        </li>
    </ol>

    <script>
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
                delimiters: [
                    {left: "$$", right: "$$", display: true},
                    {left: "\\[", right: "\\]", display: true},
                    {left: "$", right: "$", display: false},
                    {left: "\\(", right: "\\)", display: false}
                ],
                throwOnError: false
            });
        });
    </script>
</body>
</html>